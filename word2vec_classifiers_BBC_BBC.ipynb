{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import pickle\n",
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import copy\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from TurkishStemmer import TurkishStemmer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier,LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "fname=\"./data/word2vec.txt\"\n",
    "myfile=open(fname,\"r\")\n",
    "\n",
    "word2vec={}\n",
    "\n",
    "for line in myfile.readlines():\n",
    "    line=line.strip().split()\n",
    "    word2vec[line[0]]=np.fromiter((map(lambda x:float(x), line[1:])),float)\n",
    "\n",
    "myfile.close()\n",
    "\n",
    "all_dicts = pickle.load( open( \"./data/clean_data.p\", \"rb\" ) )\n",
    "\n",
    "article_text_dict_positive=all_dicts[\"article_text_dict_positive\"]\n",
    "iter1_BBC_text_dict_neg=all_dicts[\"iter1_BBC_text_dict_neg\"]\n",
    "iter2_BBC_text_dict_neg=all_dicts[\"iter2_BBC_text_dict_neg\"]\n",
    "iter1_CNN_neg_text=all_dicts[\"iter1_CNN_neg_text\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_pos 829 test_pos 208\n",
      "train_neg 834 test_neg 209\n",
      "X_train 1663 Y_train 1663 X_test 417 Y_test 417\n"
     ]
    }
   ],
   "source": [
    "pos_examp=[]\n",
    "neg_examp_train=[]\n",
    "neg_examp_test=[]\n",
    "\n",
    "for artc in article_text_dict_positive.values():\n",
    "    pos_examp.append((artc,1))\n",
    "\n",
    "for artc in iter2_BBC_text_dict_neg.values():\n",
    "    neg_examp_train.append((artc,0))\n",
    "\n",
    "for artc in iter1_CNN_neg_text:\n",
    "    neg_examp_test.append((artc,0))\n",
    "\n",
    "# combine positive and negative samples then shuffle\n",
    "# XY=pos_examp+neg_examp_train\n",
    "random.seed(a=2)\n",
    "random.shuffle(pos_examp)\n",
    "random.shuffle(neg_examp_train)\n",
    "random.shuffle(neg_examp_test)\n",
    "\n",
    "percentage=0.8\n",
    "cut_point=int(len(pos_examp)*percentage)\n",
    "train_pos=pos_examp[:cut_point]\n",
    "test_pos=pos_examp[cut_point:]\n",
    "\n",
    "cut_point=int(len(neg_examp_train)*percentage)\n",
    "train_neg=neg_examp_train[:cut_point]\n",
    "cut_point=int(len(neg_examp_train)*percentage)\n",
    "test_neg=neg_examp_train[cut_point:]\n",
    "\n",
    "print(\"train_pos\",len(train_pos),\"test_pos\",len(test_pos))\n",
    "print(\"train_neg\",len(train_neg),\"test_neg\",len(test_neg))\n",
    "\n",
    "XY_train=train_pos+train_neg\n",
    "random.shuffle(XY_train)\n",
    "X_train=[k[0] for k in XY_train]\n",
    "Y_train=[k[1] for k in XY_train]\n",
    "\n",
    "XY_test=test_pos+test_neg\n",
    "random.shuffle(XY_test)\n",
    "X_test=[k[0] for k in XY_test]\n",
    "Y_test=[k[1] for k in XY_test]\n",
    "\n",
    "print(\"X_train\",len(X_train),\"Y_train\",len(Y_train),\"X_test\",len(X_test),\"Y_test\",len(Y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_test=np.array(Y_test)\n",
    "Y_train= np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transfer document string to vectors by averaging word vectors \n",
    "# document = word2vec(tokenize(text_of_document))\n",
    "# document_vec = sum(document,1) / len(document)\n",
    "\n",
    "X_train_vec = []\n",
    "\n",
    "for X in X_train:\n",
    "    total=np.zeros(300,)\n",
    "    count=0\n",
    "    for word in nltk.word_tokenize(X):\n",
    "        count+=1\n",
    "        total+=word2vec[word]\n",
    "    total/=count\n",
    "    X_train_vec.append(total)\n",
    "\n",
    "X_test_vec = []\n",
    "\n",
    "for X in X_test:\n",
    "    total=np.zeros(300,)\n",
    "    count=0\n",
    "    for word in nltk.word_tokenize(X):\n",
    "        count+=1\n",
    "        total+=word2vec[word]\n",
    "    total/=count\n",
    "    X_test_vec.append(total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment_w_features(X_train,X_test,y_train,y_test,theclassifiers):\n",
    "\n",
    "    if type(theclassifiers)!=list:\n",
    "        theclassifiers=[theclassifiers]\n",
    "    for theclassifier in theclassifiers:\n",
    "#        initialise classifier\n",
    "        if theclassifier==MLPClassifier:\n",
    "            clf= theclassifier(max_iter=1000)\n",
    "        else:\n",
    "            clf= theclassifier()\n",
    "#     create model with tranining data\n",
    "        model = clf.fit(X_train, y_train)\n",
    "#     predict test set\n",
    "        y_preds = model.predict(X_test)\n",
    "#     create the report\n",
    "        report = classification_report( y_test, y_preds )\n",
    "    \n",
    "#     find name of the classifier for printing\n",
    "        match=re.search(r\"\\.([A-z]*)'>\",str(theclassifier))\n",
    "        match=match.group(1)\n",
    "\n",
    "        result_text=\"\\033[1m Performance report of \\033[0m \\033[92m\" + \"word2vec\" +\"\\033[0m \"\n",
    "        count=0\n",
    "#         for i in range(n_range[0],n_range[1]+1):\n",
    "#             if count!=0:\n",
    "#                 result_text+=\" and \"\n",
    "#             result_text=result_text+\"\\033[91m\"+str(i)+\"-gram\\033[0m\"\n",
    "#             count+=1\n",
    "        result_text+= \" with \\033[94m\"+match+\"\\033[0m\"\n",
    "    #     print(\"Performance report of {} {}-gram\".format(feature,n_range[1]))\n",
    "        print (result_text)\n",
    "#         prnt_scores(report)\n",
    "        print(report)\n",
    "        print(accuracy_score( y_test, y_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers_list=[LinearSVC,GaussianNB,RandomForestClassifier,AdaBoostClassifier,MLPClassifier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Performance report of \u001b[0m \u001b[92mword2vec\u001b[0m  with \u001b[94mLinearSVC\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.99      0.94       209\n",
      "           1       0.98      0.89      0.94       208\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       417\n",
      "   macro avg       0.94      0.94      0.94       417\n",
      "weighted avg       0.94      0.94      0.94       417\n",
      "\n",
      "0.9400479616306955\n",
      "\u001b[1m Performance report of \u001b[0m \u001b[92mword2vec\u001b[0m  with \u001b[94mGaussianNB\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.98      0.93       209\n",
      "           1       0.97      0.87      0.92       208\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       417\n",
      "   macro avg       0.93      0.92      0.92       417\n",
      "weighted avg       0.93      0.92      0.92       417\n",
      "\n",
      "0.920863309352518\n",
      "\u001b[1m Performance report of \u001b[0m \u001b[92mword2vec\u001b[0m  with \u001b[94mRandomForestClassifier\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.98      0.93       209\n",
      "           1       0.97      0.87      0.92       208\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       417\n",
      "   macro avg       0.93      0.92      0.92       417\n",
      "weighted avg       0.93      0.92      0.92       417\n",
      "\n",
      "0.9232613908872902\n",
      "\u001b[1m Performance report of \u001b[0m \u001b[92mword2vec\u001b[0m  with \u001b[94mAdaBoostClassifier\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94       209\n",
      "           1       0.94      0.94      0.94       208\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       417\n",
      "   macro avg       0.94      0.94      0.94       417\n",
      "weighted avg       0.94      0.94      0.94       417\n",
      "\n",
      "0.9400479616306955\n",
      "\u001b[1m Performance report of \u001b[0m \u001b[92mword2vec\u001b[0m  with \u001b[94mMLPClassifier\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97       209\n",
      "           1       0.97      0.98      0.97       208\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       417\n",
      "   macro avg       0.97      0.97      0.97       417\n",
      "weighted avg       0.97      0.97      0.97       417\n",
      "\n",
      "0.9712230215827338\n"
     ]
    }
   ],
   "source": [
    "for clasifier in classifiers_list:\n",
    "        run_experiment_w_features(X_train_vec[:],X_test_vec[:],Y_train[:],Y_test[:],clasifier)\n",
    "              precision    recall  f1-score \n",
    "\n",
    "# LinearSVC     0.94      0.94      0.94\n",
    "# GaussianNB    0.93      0.92      0.92 \n",
    "# RandomForest  0.93      0.92      0.92\n",
    "# AdaBoost      0.94      0.94      0.94\n",
    "# MLPClassifier 0.97      0.97      0.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HPCenv_conda",
   "language": "python",
   "name": "hpcenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
