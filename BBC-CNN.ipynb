{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk \n",
    "import string\n",
    "import copy\n",
    "import random\n",
    "from os import listdir\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from TurkishStemmer import TurkishStemmer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier,LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "from pos_tagger import tag\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_dicts = pickle.load( open( \"./data/clean_data.p\", \"rb\" ) )\n",
    "\n",
    "article_text_dict_positive=all_dicts[\"article_text_dict_positive\"]\n",
    "iter1_BBC_text_dict_neg=all_dicts[\"iter1_BBC_text_dict_neg\"]\n",
    "iter2_BBC_text_dict_neg=all_dicts[\"iter2_BBC_text_dict_neg\"]\n",
    "iter1_CNN_neg_text=all_dicts[\"iter1_CNN_neg_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1038\n",
      " 1043\n",
      " 948\n",
      " 1037\n"
     ]
    }
   ],
   "source": [
    "print(\"\",len(iter1_BBC_text_dict_neg))\n",
    "print(\"\",len(iter2_BBC_text_dict_neg))\n",
    "print(\"\",len(iter1_CNN_neg_text))\n",
    "print(\"\",len(article_text_dict_positive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lowercase=' abcdefghijklmnoprstuvyzğöıüşç'\n",
    "stemmer = TurkishStemmer()\n",
    "\n",
    "# There are different tokenization functions\n",
    "\n",
    "# first one is tokenizes with nltk, lowers text \n",
    "# and applies stemming to each word\n",
    "def tokenize_stem(text):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems text. Returns a list of them\"\"\"\n",
    "    text=nltk.word_tokenize(text.lower().strip())\n",
    "    tokens = [stemmer.stem(t) for t in text]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# lowers text\n",
    "# generates part of speech tags  \n",
    "# returns \"word+tag\" \n",
    "# tokenization made with tagging library\n",
    "def pos_tokenize(text):\n",
    "    text=text.lower().strip()\n",
    "    tokens=[]\n",
    "    tags=tag(text)\n",
    "    for a_tag in tags:\n",
    "        tokens.append(a_tag[0]+\"+\"+a_tag[1])\n",
    "    return tokens\n",
    "\n",
    "# lowers text\n",
    "# splits text to it's characters\n",
    "def char_tokenize(text):\n",
    "    text = text.lower().strip()\n",
    "    tokens = [t for t in text]\n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of feature functions to be tested \n",
    "features={\"Pos_tags\":pos_tokenize,\"characters\":char_tokenize,\"tokenize\":nltk.word_tokenize,\"tokenize_stem\":tokenize_stem}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment_w_features(X_train,X_test,y_train,y_test,feature,n_range,theclassifiers,vectorizer=None):\n",
    "    if vectorizer==None:\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            tokenizer= features[feature],\n",
    "#             cleaning itself\n",
    "#             preprocessor=word_process_clean,\n",
    "            ngram_range=n_range,\n",
    "            use_idf=True,\n",
    "            min_df=0.003,\n",
    "            norm=None, )\n",
    "        \n",
    "    \n",
    "    X_train = vectorizer.fit_transform(X_train).toarray()\n",
    "    X_test = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "    if type(theclassifiers)!=list:\n",
    "        theclassifiers=[theclassifiers]\n",
    "    for theclassifier in theclassifiers:\n",
    "#       initialise classifier\n",
    "        if theclassifier==LinearSVC:\n",
    "            clf= theclassifier(max_iter=-1)\n",
    "        else:\n",
    "            clf= theclassifier()\n",
    "#     create model with tranining data\n",
    "        model = clf.fit(X_train, y_train)\n",
    "#     predict test set\n",
    "        y_preds = model.predict(X_test)\n",
    "#     create the report\n",
    "        report = classification_report( y_test, y_preds )\n",
    "    \n",
    "#     find name of the classifier for printing\n",
    "        match=re.search(r\"\\.([A-z]*)'>\",str(theclassifier))\n",
    "        match=match.group(1)\n",
    "\n",
    "        result_text=\"\\033[1m Performance report of \\033[0m \\033[92m\" + feature +\"\\033[0m \"\n",
    "        count=0\n",
    "        for i in range(n_range[0],n_range[1]+1):\n",
    "            if count!=0:\n",
    "                result_text+=\" and \"\n",
    "            result_text=result_text+\"\\033[91m\"+str(i)+\"-gram\\033[0m\"\n",
    "            count+=1\n",
    "        result_text+= \" with \\033[94m\"+match+\"\\033[0m\"\n",
    "    #     print(\"Performance report of {} {}-gram\".format(feature,n_range[1]))\n",
    "        print (result_text)\n",
    "#         prnt_scores(report)\n",
    "        print(report)\n",
    "        print(accuracy_score( y_test, y_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos examples 1037\n",
      "BBC 1043\n",
      "CNN 948\n",
      "pos examples 1037\n",
      "BBC 1037\n",
      "CNN 948\n",
      "train_pos 829 test_pos 208\n",
      "train_neg 834 test_neg 190\n",
      "X_train 1663 Y_train 1663 X_test 398 Y_test 398\n"
     ]
    }
   ],
   "source": [
    "pos_examp=[]\n",
    "neg_examp_train=[]\n",
    "neg_examp_test=[]\n",
    "\n",
    "print(\"pos examples\",len(article_text_dict_positive))\n",
    "print(\"BBC\",len(iter2_BBC_text_dict_neg))\n",
    "print(\"CNN\",len(iter1_CNN_neg_text))\n",
    "\n",
    "# some articles includes short text \n",
    "for artc in article_text_dict_positive.values():\n",
    "    pos_examp.append((artc,1))\n",
    "\n",
    "for artc in iter2_BBC_text_dict_neg.values():\n",
    "    neg_examp_train.append((artc,0))\n",
    "\n",
    "for artc in iter1_CNN_neg_text.values():\n",
    "    neg_examp_test.append((artc,0))\n",
    "\n",
    "print(\"pos examples\",len(pos_examp))\n",
    "print(\"BBC\",len(pos_examp))\n",
    "print(\"CNN\",len(neg_examp_test))\n",
    "\n",
    "# combine positive and negative samples then shuffle\n",
    "# XY=pos_examp+neg_examp_train\n",
    "random.seed(a=2)\n",
    "random.shuffle(pos_examp)\n",
    "random.shuffle(neg_examp_train)\n",
    "random.shuffle(neg_examp_test)\n",
    "\n",
    "percentage=0.8\n",
    "cut_point=int(len(pos_examp)*percentage)\n",
    "train_pos=pos_examp[:cut_point]\n",
    "test_pos=pos_examp[cut_point:]\n",
    "\n",
    "cut_point=int(len(neg_examp_train)*percentage)\n",
    "train_neg=neg_examp_train[:cut_point]\n",
    "cut_point=int(len(neg_examp_test)*percentage)\n",
    "test_neg=neg_examp_test[cut_point:]\n",
    "\n",
    "print(\"train_pos\",len(train_pos),\"test_pos\",len(test_pos))\n",
    "print(\"train_neg\",len(train_neg),\"test_neg\",len(test_neg))\n",
    "\n",
    "XY_train=train_pos+train_neg\n",
    "random.shuffle(XY_train)\n",
    "X_train=[k[0] for k in XY_train]\n",
    "Y_train=[k[1] for k in XY_train]\n",
    "\n",
    "XY_test=test_pos+test_neg\n",
    "random.shuffle(XY_test)\n",
    "X_test=[k[0] for k in XY_test]\n",
    "Y_test=[k[1] for k in XY_test]\n",
    "\n",
    "\n",
    "print(\"X_train\",len(X_train),\"Y_train\",len(Y_train),\"X_test\",len(X_test),\"Y_test\",len(Y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers_list=[LinearSVC,MultinomialNB,RandomForestClassifier,\n",
    "                  AdaBoostClassifier]\n",
    "tryfeatures={\"tokenize\":nltk.word_tokenize,\"tokenize_stem\":tokenize_stem,\"Pos_tags\":pos_tokenize,\"characters\":char_tokenize}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/HPCenv/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/envs/HPCenv/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Performance report of \u001b[0m \u001b[92mtokenize\u001b[0m \u001b[91m1-gram\u001b[0m with \u001b[94mLinearSVC\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      1.00      0.65       190\n",
      "           1       0.00      0.00      0.00       208\n",
      "\n",
      "   micro avg       0.48      0.48      0.48       398\n",
      "   macro avg       0.24      0.50      0.32       398\n",
      "weighted avg       0.23      0.48      0.31       398\n",
      "\n",
      "0.47738693467336685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/HPCenv/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/envs/HPCenv/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Performance report of \u001b[0m \u001b[92mtokenize\u001b[0m \u001b[91m1-gram\u001b[0m and \u001b[91m2-gram\u001b[0m with \u001b[94mLinearSVC\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      1.00      0.65       190\n",
      "           1       0.00      0.00      0.00       208\n",
      "\n",
      "   micro avg       0.48      0.48      0.48       398\n",
      "   macro avg       0.24      0.50      0.32       398\n",
      "weighted avg       0.23      0.48      0.31       398\n",
      "\n",
      "0.47738693467336685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/HPCenv/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/envs/HPCenv/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Performance report of \u001b[0m \u001b[92mtokenize\u001b[0m \u001b[91m1-gram\u001b[0m and \u001b[91m2-gram\u001b[0m and \u001b[91m3-gram\u001b[0m with \u001b[94mLinearSVC\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      1.00      0.65       190\n",
      "           1       0.00      0.00      0.00       208\n",
      "\n",
      "   micro avg       0.48      0.48      0.48       398\n",
      "   macro avg       0.24      0.50      0.32       398\n",
      "weighted avg       0.23      0.48      0.31       398\n",
      "\n",
      "0.47738693467336685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/HPCenv/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/envs/HPCenv/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Performance report of \u001b[0m \u001b[92mtokenize\u001b[0m \u001b[91m2-gram\u001b[0m with \u001b[94mLinearSVC\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      1.00      0.65       190\n",
      "           1       0.00      0.00      0.00       208\n",
      "\n",
      "   micro avg       0.48      0.48      0.48       398\n",
      "   macro avg       0.24      0.50      0.32       398\n",
      "weighted avg       0.23      0.48      0.31       398\n",
      "\n",
      "0.47738693467336685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/HPCenv/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/envs/HPCenv/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Performance report of \u001b[0m \u001b[92mtokenize\u001b[0m \u001b[91m2-gram\u001b[0m and \u001b[91m3-gram\u001b[0m with \u001b[94mLinearSVC\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      1.00      0.65       190\n",
      "           1       0.00      0.00      0.00       208\n",
      "\n",
      "   micro avg       0.48      0.48      0.48       398\n",
      "   macro avg       0.24      0.50      0.32       398\n",
      "weighted avg       0.23      0.48      0.31       398\n",
      "\n",
      "0.47738693467336685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/HPCenv/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/envs/HPCenv/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Performance report of \u001b[0m \u001b[92mtokenize\u001b[0m \u001b[91m3-gram\u001b[0m with \u001b[94mLinearSVC\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      1.00      0.65       190\n",
      "           1       0.00      0.00      0.00       208\n",
      "\n",
      "   micro avg       0.48      0.48      0.48       398\n",
      "   macro avg       0.24      0.50      0.32       398\n",
      "weighted avg       0.23      0.48      0.31       398\n",
      "\n",
      "0.47738693467336685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/HPCenv/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/envs/HPCenv/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Performance report of \u001b[0m \u001b[92mtokenize_stem\u001b[0m \u001b[91m1-gram\u001b[0m with \u001b[94mLinearSVC\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      1.00      0.65       190\n",
      "           1       0.00      0.00      0.00       208\n",
      "\n",
      "   micro avg       0.48      0.48      0.48       398\n",
      "   macro avg       0.24      0.50      0.32       398\n",
      "weighted avg       0.23      0.48      0.31       398\n",
      "\n",
      "0.47738693467336685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/HPCenv/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/envs/HPCenv/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Performance report of \u001b[0m \u001b[92mtokenize_stem\u001b[0m \u001b[91m1-gram\u001b[0m and \u001b[91m2-gram\u001b[0m with \u001b[94mLinearSVC\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      1.00      0.65       190\n",
      "           1       0.00      0.00      0.00       208\n",
      "\n",
      "   micro avg       0.48      0.48      0.48       398\n",
      "   macro avg       0.24      0.50      0.32       398\n",
      "weighted avg       0.23      0.48      0.31       398\n",
      "\n",
      "0.47738693467336685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/HPCenv/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/envs/HPCenv/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Performance report of \u001b[0m \u001b[92mtokenize_stem\u001b[0m \u001b[91m1-gram\u001b[0m and \u001b[91m2-gram\u001b[0m and \u001b[91m3-gram\u001b[0m with \u001b[94mLinearSVC\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      1.00      0.65       190\n",
      "           1       0.00      0.00      0.00       208\n",
      "\n",
      "   micro avg       0.48      0.48      0.48       398\n",
      "   macro avg       0.24      0.50      0.32       398\n",
      "weighted avg       0.23      0.48      0.31       398\n",
      "\n",
      "0.47738693467336685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/HPCenv/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/envs/HPCenv/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Performance report of \u001b[0m \u001b[92mtokenize_stem\u001b[0m \u001b[91m2-gram\u001b[0m with \u001b[94mLinearSVC\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      1.00      0.65       190\n",
      "           1       0.00      0.00      0.00       208\n",
      "\n",
      "   micro avg       0.48      0.48      0.48       398\n",
      "   macro avg       0.24      0.50      0.32       398\n",
      "weighted avg       0.23      0.48      0.31       398\n",
      "\n",
      "0.47738693467336685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/HPCenv/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/envs/HPCenv/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Performance report of \u001b[0m \u001b[92mtokenize_stem\u001b[0m \u001b[91m2-gram\u001b[0m and \u001b[91m3-gram\u001b[0m with \u001b[94mLinearSVC\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      1.00      0.65       190\n",
      "           1       0.00      0.00      0.00       208\n",
      "\n",
      "   micro avg       0.48      0.48      0.48       398\n",
      "   macro avg       0.24      0.50      0.32       398\n",
      "weighted avg       0.23      0.48      0.31       398\n",
      "\n",
      "0.47738693467336685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/HPCenv/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/envs/HPCenv/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Performance report of \u001b[0m \u001b[92mtokenize_stem\u001b[0m \u001b[91m3-gram\u001b[0m with \u001b[94mLinearSVC\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      1.00      0.65       190\n",
      "           1       0.00      0.00      0.00       208\n",
      "\n",
      "   micro avg       0.48      0.48      0.48       398\n",
      "   macro avg       0.24      0.50      0.32       398\n",
      "weighted avg       0.23      0.48      0.31       398\n",
      "\n",
      "0.47738693467336685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/HPCenv/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/envs/HPCenv/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Performance report of \u001b[0m \u001b[92mPos_tags\u001b[0m \u001b[91m1-gram\u001b[0m with \u001b[94mLinearSVC\u001b[0m\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      1.00      0.65       190\n",
      "           1       0.00      0.00      0.00       208\n",
      "\n",
      "   micro avg       0.48      0.48      0.48       398\n",
      "   macro avg       0.24      0.50      0.32       398\n",
      "weighted avg       0.23      0.48      0.31       398\n",
      "\n",
      "0.47738693467336685\n"
     ]
    }
   ],
   "source": [
    "for clasifier in classifiers_list:\n",
    "    for feature in tryfeatures:\n",
    "        for i in range(1,4):\n",
    "            for k in range(i,4):\n",
    "                run_experiment_w_features(X_train[:],X_test[:],Y_train[:],Y_test[:],feature,(i,k),clasifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_classifiers_list=[DecisionTreeClassifier,GaussianNB,MLPClassifier]\n",
    "for clasifier in slow_classifiers_list:\n",
    "    for feature in tryfeatures:\n",
    "        for i in range(1,4):\n",
    "            run_experiment_w_features(X_train[:],X_test[:],Y_train[:],Y_test[:],feature,(i,i),clasifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HPCenv_conda",
   "language": "python",
   "name": "hpcenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
