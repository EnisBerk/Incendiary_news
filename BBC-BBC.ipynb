{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk \n",
    "import string\n",
    "import copy\n",
    "import random\n",
    "from os import listdir\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from TurkishStemmer import TurkishStemmer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier,LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "from pos_tagger import tag\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# there are four files with dataset, because I collected data in different times\n",
    "# they are dictionaries, keys are links to news and text of the news is value\n",
    "\n",
    "text_store=open(\"article_text_pos_1063_57531copy.json\",\"r\")\n",
    "article_text_dict_positive = json.load(text_store)\n",
    "text_store.close()\n",
    "# '3815', \"text\"\n",
    "\n",
    "# new negative samples from BBC are :\n",
    "text_store=open(\"article_text_neg_bbc_iter1_07272.json\",\"r\")\n",
    "iter1_BBC_text_dict_neg = json.load(text_store)\n",
    "text_store.close()\n",
    "\n",
    "text_store=open(\"iter2_text_neg_bbc_12981.json\",\"r\")\n",
    "iter2_BBC_text_dict_neg = json.load(text_store)\n",
    "text_store.close()\n",
    "\n",
    "text_store=open(\"iter1_article_text_neg_CNN_08109.json\",\"r\")\n",
    "iter1_CNN_neg_text = json.load(text_store)\n",
    "text_store.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toremove=[]\n",
    "for asd in article_text_dict_positive.items():\n",
    "    if len(asd[1])<100:\n",
    "        toremove.append(asd[0])\n",
    "for keys in toremove:\n",
    "    del article_text_dict_positive[keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1038\n",
      "1043\n",
      "948\n",
      "1037\n"
     ]
    }
   ],
   "source": [
    "print(len(iter1_BBC_text_dict_neg))\n",
    "print(len(iter2_BBC_text_dict_neg))\n",
    "print(len(iter1_CNN_neg_text))\n",
    "print(len(article_text_dict_positive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#those stop words to be removed\n",
    "my_stopwords = stopwords.words('turkish')\n",
    "\n",
    "# those are features that are not related to hate-speech so they will be removed\n",
    "my_stopwords.extend(['facebook', 'telif','hakkı', 'telif hakkı','dr','bbc','bır','dan', 'den',\n",
    "                    'karsı','twitter',\"caption\",\"image\",\n",
    "                    \"image caption\",\"tiklayin\",'copyright',\n",
    "                     'yayın','sayfa','no',\n",
    "                     \"tüm\",\"hakları\",'saklıdır','copyright'\n",
    "                    \"sayfa\",\"world\",\"world service\",\"yeni akit\",\"gazetesihalkalı\",\n",
    "                  \"tüm hakları saklıdır\",\"hakları saklıdır\",\"hakları\",'saklıdır',\n",
    "                    \"yayın\",\"sayfa\",\"no\",'ve',\"cnn\",\"aa\",\"destekyeniakitcomtr\",\"httpyeniakitcomtr\"])\n",
    "\n",
    "# some websites tends to use special characters too much, remove them all\n",
    "removethose='“’‘”•.,\\'\\\"!;@?())'\n",
    "removethose+='*'+ '0'+ ':'+ ']'+ '_'+'$'+ '['+'{'+ '}'+ '»'\n",
    "removethose+='©'+'-'+\"1234567890\"+\"&\"+\"—\"+\"/\"+\"|\"+\"=\"+\">\"+\"…\"+\"%\"+\"′\"\n",
    "\n",
    "removethose=['“', '’', '‘', '”', '•', '.', ',', \"'\", '\"', '!', \n",
    "             ';', '@', '?', '(', ')', ')', '*', '0', ':', ']',\n",
    "             '_', '$', '[', '{', '}', '»', '©', '-', '1', '2', \n",
    "             '3', '4', '5', '6', '7', '8', '9', '0', '&', '—', \n",
    "             '/', '|', '=', '>', '…', '%', '′','€', '¥','£','›',\n",
    "             '¼','<','¨','‏','­','–','#','+']\n",
    "\n",
    "lowercase=' abcdefghijklmnoprstuvyzğöıüşç'\n",
    "# Turkish stemmer from https://github.com/otuncelli/turkish-stemmer-python\n",
    "stemmer = TurkishStemmer()\n",
    "\n",
    "# this function lowers text, \n",
    "# then removes characters such as @ or » because\n",
    "# those are used in some resources of news more than others and\n",
    "# also corrects some characters such as 'i̇' which\n",
    "# does not exists in Turkish but found in the text\n",
    "# due to encoding or scraping errors\n",
    "def pre_process_clean_old(text):\n",
    "    text=text.lower()\n",
    "    text=text.strip()\n",
    "    for rmt in removethose:\n",
    "        text=text.replace(rmt,\"\")\n",
    "    text=text.replace('i̇','i').replace('î','i').replace('â','a').replace('á','a')\n",
    "    text=text.replace('ū','ü').replace('û','u')\n",
    "    text=text.replace('è','e').replace('é','e').replace('ê','e')\n",
    "    return text\n",
    "\n",
    "def pre_process_clean(text):\n",
    "    text=text.lower()\n",
    "    text=text.strip()\n",
    "    text=text.replace('i̇','i').replace('î','i').replace('â','a').replace('á','a')\n",
    "    text=text.replace('ū','ü').replace('û','u')\n",
    "    text=text.replace('è','e').replace('é','e').replace('ê','e')\n",
    "    result=\"\"\n",
    "    for thechar in text:\n",
    "        if (thechar in lowercase):\n",
    "            result+=thechar\n",
    "    return result\n",
    "\n",
    "def pre_process_stopwords(text):\n",
    "    text=nltk.word_tokenize(text.lower().strip())\n",
    "    text=filter(lambda x: x not in my_stopwords,text )\n",
    "    text=\" \".join(text)\n",
    "    return text\n",
    "\n",
    "# There are different tokenization functions\n",
    "\n",
    "# first one is tokenizes with nltk, lowers text \n",
    "# and applies stemming to each word\n",
    "def tokenize_stem(text):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems text. Returns a list of them\"\"\"\n",
    "    text=nltk.word_tokenize(text.lower().strip())\n",
    "    tokens = [stemmer.stem(t) for t in text]\n",
    "    return tokens\n",
    "\n",
    "# lowers text, uses tagging library's tokenization \n",
    "# NO-NEED\n",
    "# def tokenize(text):\n",
    "#     text=text.lower().strip()\n",
    "#     tokens=[]\n",
    "#     tags=tag(text)\n",
    "#     for a_tag in tags:\n",
    "#         if a_tag[0] not in my_stopwords:\n",
    "#             tokens.append(a_tag[0])\n",
    "#     return tokens\n",
    "\n",
    "# lowers text\n",
    "# generates part of speech tags  \n",
    "# returns \"word+tag\" \n",
    "# tokenization made with tagging library\n",
    "def pos_tokenize(text):\n",
    "    text=text.lower().strip()\n",
    "    tokens=[]\n",
    "    tags=tag(text)\n",
    "    for a_tag in tags:\n",
    "        tokens.append(a_tag[0]+\"+\"+a_tag[1])\n",
    "    return tokens\n",
    "\n",
    "# lowers text\n",
    "# splits text to it's characters\n",
    "def char_tokenize(text):\n",
    "    text = text.lower().strip()\n",
    "    tokens = [t for t in text]\n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features={\"Pos_tags\":pos_tokenize,\"characters\":char_tokenize,\"tokenize\":nltk.word_tokenize,\"tokenize_stem\":tokenize_stem}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment_w_features(X_train,X_test,y_train,y_test,feature,n_range,theclassifiers,vectorizer=None):\n",
    "    if vectorizer==None:\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            tokenizer= features[feature],\n",
    "#             cleaning itself\n",
    "#             preprocessor=word_process_clean,\n",
    "            ngram_range=n_range,\n",
    "            use_idf=True,\n",
    "            min_df=0.003,\n",
    "            norm=None, )\n",
    "        \n",
    "    \n",
    "    X_train = vectorizer.fit_transform(X_train).toarray()\n",
    "    X_test=vectorizer.transform(X_test).toarray()\n",
    "\n",
    "#     y=Y[:]\n",
    "    if type(theclassifiers)!=list:\n",
    "        theclassifiers=[theclassifiers]\n",
    "    for theclassifier in theclassifiers:\n",
    "#       initialise classifier\n",
    "        clf= theclassifier()\n",
    "#     create model with tranining data\n",
    "        model = clf.fit(X_train, y_train)\n",
    "#     predict test set\n",
    "        y_preds = model.predict(X_test)\n",
    "#     create the report\n",
    "        report = classification_report( y_test, y_preds )\n",
    "    \n",
    "#     find name of the classifier for printing\n",
    "        match=re.search(r\"\\.([A-z]*)'>\",str(theclassifier))\n",
    "        match=match.group(1)\n",
    "\n",
    "        result_text=\"\\033[1m Performance report of \\033[0m \\033[92m\" + feature +\"\\033[0m \"\n",
    "        count=0\n",
    "        for i in range(n_range[0],n_range[1]+1):\n",
    "            if count!=0:\n",
    "                result_text+=\" and \"\n",
    "            result_text=result_text+\"\\033[91m\"+str(i)+\"-gram\\033[0m\"\n",
    "            count+=1\n",
    "        result_text+= \" with \\033[94m\"+match+\"\\033[0m\"\n",
    "    #     print(\"Performance report of {} {}-gram\".format(feature,n_range[1]))\n",
    "        print (result_text)\n",
    "#         prnt_scores(report)\n",
    "        print(report)\n",
    "        print(accuracy_score( y_test, y_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pre_process_clean(a_text)\n",
    "for a_dict in [article_text_dict_positive,iter1_BBC_text_dict_neg,iter2_BBC_text_dict_neg,\n",
    "iter1_CNN_neg_text]:\n",
    "    for key,value in a_dict.items():\n",
    "        a_dict[key]=pre_process_stopwords(pre_process_clean(value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['3815', '3575', '3392', '3241', '3156', '3012', '2910', '2803', '2757', '2643', '2635', '2599', '2582', '2556', '2439', '2438', '2437', '2345', '2339', '2330', '2326', '2269', '2268', '2259', '2224', '2209', '2195', '2187', '2162', '2148', '2123', '2117', '2112', '2109', '2108', '2103', '2098', '2084', '2081', '2071', '2067', '2061', '2058', '2051', '2044', '2043', '2021', '1946', '1937', '1929', '1920', '1900', '1897', '1838', '1835', '1818', '1814', '1808', '1804', '1792', '1764', '1733', '1728', '1705', '1701', '1695', '1693', '1677', '1676', '1665', '1649', '1645', '1624', '1623', '1603', '1583', '1580', '1570', '1566', '1555', '1544', '1490', '1489', '1453', '1446', '1442', '1432', '1416', '1403', '1387', '1358', '1315', '1308', '1302', '1297', '1217', '1199', '1086', '1064', '1036', '1030', '1002', '998', '977', '975', '954', '934', '873', '842', '838', '795', '794', '744', '730', '685', '671', '636', '433', '313', '303', '283', '281', '263', '256', '254', '234', '3878', '3855', '3814', '3763', '3736', '3691', '3650', '3636', '3629', '3610', '3530', '3488', '3461', '3456', '3451', '3428', '3395', '3286', '3212', '3206', '3203', '3189', '3171', '3160', '3109', '3060', '3021', '3013', '2996', '2994', '2992', '2963', '2960', '2959', '2849', '2804', '2674', '2645', '3896', '3895', '3891', '3890', '3889', '3888', '3879', '3876', '3871', '3863', '3851', '3867', '3860', '3807', '3806', '3790', '3787', '3786', '3783', '3782', '3897', '3900', '3902', '3906', '3908', '3111', '3063', '3211', '3133', '2421', '2083', '948', '3258', '2976', '2912', '3134', '1384', '1711', '3040', '3346', '2086', '2644', '3615', '3578', '2447', '2539', '554', '1113', '3605', '2448', '2606', '2977', '208', '252', '1731', '2213', '1242', '3130', '1904', '1721', '3676', '2094', '2700', '1454', '2630', '2560', '3805', '1316', '2792', '1205', '1426', '3019', '935', '3015', '2610', '1992', '3227', '3070', '3256', '3491', '2952', '308', '2577', '2155', '1708', '3762', '3172', '3550', '206', '2492', '2361', '2055', '3620', '2550', '1572', '2159', '3073', '3795', '1385', '2018', '2342', '2585', '3524', '3714', '2499', '2446', '2146', '2584', '962', '3045', '2921', '3264', '2360', '530', '2936', '3834', '989', '786', '2404', '1748', '1448', '3655', '2299', '2455', '2497', '3718', '2659', '3614', '2175', '2328', '2329', '3682', '3038', '3412', '2282', '1894', '1273', '1320', '3024', '1789', '2211', '3740', '635', '2318', '3331', '1587', '3339', '1702', '1858', '3754', '3414', '2435', '3410', '2973', '1955', '1771', '2969', '1967', '277', '1724', '1332', '245', '951', '2948', '3477', '2464', '3075', '2528', '2743', '2364', '2536', '2896', '1289', '2493', '3480', '2724', '1304', '1796', '1467', '3061', '2564', '2920', '1482', '3498', '987', '3127', '2892', '1531', '3818', '3224', '1997', '1524', '2143', '532', '1391', '2479', '3110', '2926', '1928', '3336', '2182', '1535', '2701', '2465', '2380', '3164', '656', '979', '3239', '2903', '3055', '1901', '389', '3826', '1395', '1713', '1574', '3026', '2988', '2215', '649', '974', '3706', '1504', '1805', '2434', '3421', '2469', '3509', '1706', '829', '3744', '968', '2754', '2144', '3848', '355', '2114', '1088', '1148', '857', '3380', '969', '3103', '2459', '3158', '2285', '1402', '2501', '3165', '2879', '2980', '3204', '1259', '575', '3143', '2982', '2524', '3233', '2816', '1414', '1439', '1933', '787', '2057', '2696', '1154', '3430', '260', '1073', '1060', '876', '3580', '3854', '2423', '3508', '3904', '2189', '3081', '980', '1043', '552', '1390', '3356', '3340', '3905', '2989', '2883', '1203', '2066', '2425', '3308', '3291', '1144', '2957', '2200', '1810', '2237', '1092', '1257', '3796', '3183', '3537', '3102', '1141', '2686', '3454', '3219', '1235', '1080', '3001', '1836', '3784', '3065', '1000', '2904', '32', '1650', '659', '262', '2091', '3153', '3565', '2194', '2280', '2490', '3747', '361', '3168', '2045', '1147', '3601', '2559', '327', '3290', '2558', '362', '1593', '3764', '2132', '3596', '3730', '1428', '1783', '3621', '3360', '42', '2967', '1777', '2568', '1847', '365', '3108', '3093', '2949', '2185', '2662', '3644', '3186', '1996', '2292', '1921', '3598', '3640', '2958', '2140', '2770', '1671', '1492', '2277', '3460', '1325', '2427', '3259', '3027', '2153', '3378', '1045', '339', '1974', '1537', '2431', '2515', '1767', '1164', '2747', '320', '945', '305', '1632', '3877', '2924', '2594', '1463', '152', '2500', '3473', '441', '2236', '2398', '1655', '2279', '3833', '502', '3299', '2157', '3342', '2118', '386', '2024', '2538', '2940', '2308', '2343', '3612', '1267', '687', '3370', '3717', '2273', '2121', '3593', '3096', '1486', '2880', '3396', '3223', '3838', '1906', '1054', '3349', '1854', '1681', '1488', '905', '2177', '239', '1420', '1458', '1958', '1819', '1809', '1464', '333', '3749', '1618', '3528', '2180', '3029', '1196', '297', '3722', '3384', '2516', '269', '3651', '1170', '2311', '2725', '1827', '1360', '3382', '2107', '1679', '1982', '3446', '1474', '2496', '981', '1218', '3638', '1772', '1849', '1675', '1960', '3559', '2688', '1419', '3031', '3352', '2590', '2764', '1823', '2705', '1163', '160', '1990', '3841', '1056', '3555', '1548', '3250', '2506', '1480', '1456', '134', '3162', '2714', '2441', '3379', '2193', '2588', '1760', '1800', '1571', '1097', '3409', '1934', '2152', '2060', '2016', '1856', '3267', '588', '3661', '3208', '2944', '3518', '3323', '1586', '158', '317', '940', '3309', '1986', '312', '1875', '2627', '3120', '946', '3316', '1857', '558', '598', '927', '3242', '3128', '992', '2017', '3513', '3821', '2753', '1047', '3369', '2025', '2110', '2252', '1435', '1754', '3431', '1122', '2190', '1718', '1431', '3126', '279', '3176', '3326', '963', '2888', '2498', '621', '1757', '3712', '2693', '2221', '3802', '3080', '3835', '3155', '1565', '2119', '2561', '3005', '3140', '3794', '2007', '1961', '2196', '3436', '3121', '1822', '1231', '1678', '1802', '3394', '3684', '3003', '3039', '3033', '1552', '2628', '2916', '1393', '2093', '3294', '3338', '3611', '3850', '3318', '1755', '737', '1195', '3426', '1516', '2638', '1536', '2709', '3830', '1725', '1145', '2546', '136', '3288', '2444', '2334', '3225', '1911', '2597', '2178', '3194', '2080', '978', '2587', '3637', '1801', '1005', '1532', '3180', '3229', '1543', '3147', '3328', '2262', '2316', '1925', '2064', '952', '3556', '1932', '3548', '3564', '3305', '3585', '1687', '3136', '3839', '1924', '1902', '955', '3401', '1048', '2673', '1872', '1336', '340', '3095', '2156', '3481', '3434', '1852', '3129', '1720', '1546', '3635', '2288', '2150', '3090', '336', '3062', '39', '2902', '3341', '2557', '767', '3275', '1418', '1445', '3885', '2097', '2138', '2298', '1153', '1509', '1943', '2402', '2472', '3246', '1262', '1451', '3372', '2508', '3589', '2491', '3466', '2734', '3861', '3302', '3514', '967', '2347', '2730', '2717', '3869', '2403', '3771', '1613', '1371', '3310', '2022', '1832', '2102', '2918', '2789', '3056', '1941', '2293', '3150', '1150', '2961', '1476', '576', '2595', '727', '3628', '937', '1756', '2576', '1578', '2567', '2943', '1743', '1669', '456', '2741', '3301', '2099', '1079', '176', '2476', '1600', '1595', '2040', '2407', '2723', '2217', '2445', '3849', '1821', '2320', '2056', '3699', '200', '2797', '3545', '3734', '2570', '3319', '2744', '2474', '2698', '2164', '2721', '2428', '2220', '3438', '841', '566', '2986', '2137', '3182', '3351', '2325', '3653', '2716', '2161', '936', '2591', '3420', '2565', '1438', '1579', '2408', '3478', '2971', '1564', '2640', '2087', '3007', '956', '1165', '2953', '3557', '1011', '2633', '1820', '3489', '1280', '1799', '3715', '2388', '582', '2432', '2336', '1028', '3515', '1366', '2104', '2796', '3300', '3413', '2521', '2278', '3776', '3415', '2915', '3847', '1716', '2682', '3566', '3251', '2917', '2801', '1470', '2641', '1408', '1641', '167', '2708', '304', '563', '2461', '487', '2671', '994', '3209', '1233', '2615', '3727', '3377', '3698', '3185', '2422', '3442', '3004', '2249', '1362', '1962', '3118', '3170', '3175', '3141', '3810', '3774', '2127', '3307', '1523', '2214', '2972', '2147', '2699', '3679', '3812', '3249', '2950', '2489', '2909', '1752', '2354', '579', '335', '2667', '2454'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for key,value in article_text_dict_positive.items():\n",
    "#     print (key,value)\n",
    "#     break\n",
    "article_text_dict_positive.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_pos 828 test_pos 208\n",
      "train_neg 824 test_neg 207\n",
      "X_train 1652 Y_train 1652 X_test 415 Y_test 415\n"
     ]
    }
   ],
   "source": [
    "pos_examp=[]\n",
    "neg_examp_train=[]\n",
    "neg_examp_test=[]\n",
    "\n",
    "for artc in article_text_dict_positive.values():\n",
    "    if len(artc)>200:\n",
    "        pos_examp.append((artc,1))\n",
    "\n",
    "for artc in iter2_BBC_text_dict_neg.values():\n",
    "    if len(artc)>200:\n",
    "        neg_examp_train.append((artc,0))\n",
    "\n",
    "for artc in iter1_CNN_neg_text.values():\n",
    "    if len(artc)>200:\n",
    "        neg_examp_test.append((artc,0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# combine positive and negative samples then shuffle\n",
    "# XY=pos_examp+neg_examp_train\n",
    "random.seed(a=2)\n",
    "random.shuffle(pos_examp)\n",
    "random.shuffle(neg_examp_train)\n",
    "random.shuffle(neg_examp_test)\n",
    "\n",
    "percentage=0.8\n",
    "cut_point=int(len(pos_examp)*percentage)\n",
    "train_pos=pos_examp[:cut_point]\n",
    "test_pos=pos_examp[cut_point:]\n",
    "\n",
    "cut_point=int(len(neg_examp_train)*percentage)\n",
    "train_neg=neg_examp_train[:cut_point]\n",
    "cut_point=int(len(neg_examp_train)*percentage)\n",
    "test_neg=neg_examp_train[cut_point:]\n",
    "\n",
    "print(\"train_pos\",len(train_pos),\"test_pos\",len(test_pos))\n",
    "print(\"train_neg\",len(train_neg),\"test_neg\",len(test_neg))\n",
    "\n",
    "XY_train=train_pos+train_neg\n",
    "random.shuffle(XY_train)\n",
    "X_train=[k[0] for k in XY_train]\n",
    "Y_train=[k[1] for k in XY_train]\n",
    "\n",
    "XY_test=test_pos+test_neg\n",
    "random.shuffle(XY_test)\n",
    "X_test=[k[0] for k in XY_test]\n",
    "Y_test=[k[1] for k in XY_test]\n",
    "\n",
    "print(\"X_train\",len(X_train),\"Y_train\",len(Y_train),\"X_test\",len(X_test),\"Y_test\",len(Y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers_list=[LinearSVC,MultinomialNB,RandomForestClassifier,\n",
    "                  AdaBoostClassifier]\n",
    "tryfeatures={\"tokenize\":nltk.word_tokenize,\"tokenize_stem\":tokenize_stem,\"Pos_tags\":pos_tokenize,\"characters\":char_tokenize}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1031"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_examp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for clasifier in classifiers_list:\n",
    "    for feature in tryfeatures:\n",
    "        for i in range(1,4):\n",
    "            for k in range(i,4):\n",
    "                run_experiment_w_features(X_train[:],X_test[:],Y_train[:],Y_test[:],feature,(i,k),clasifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slow_classifiers_list=[DecisionTreeClassifier,GaussianNB,MLPClassifier]\n",
    "for clasifier in slow_classifiers_list:\n",
    "    for feature in tryfeatures:\n",
    "        for i in range(1,4):\n",
    "            run_experiment_w_features(X_train[:],X_test[:],Y_train[:],Y_test[:],feature,(i,i),clasifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myfile=open(\"results/4-(BBC-CNN).txt\")\n",
    "lines=myfile.readlines()\n",
    "myfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(*lines[9:18])\n",
    "print(*lines[18:27])\n",
    "print(lines[18].strip()[23:].split(\" \")[0])\n",
    "print(lines[9].strip()[23:].split(\" and \"))\n",
    "print(lines[9].strip()[23:].split(\" \")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(lines),9):\n",
    "    one_line=lines[i:i+9]\n",
    "    first_line=one_line[0].strip()[23:]\n",
    "    ngram=[]\n",
    "    ngrams=first_line.split(\" and \")\n",
    "    if len(ngrams)==2:\n",
    "        clasifier=first_line.split(\"with\")[1].strip()\n",
    "        ngram.append(ngrams[0][-6:])\n",
    "        ngram.append(ngrams[1][:6])\n",
    "        first_line=first_line.split(\" \")\n",
    "\n",
    "    elif len(ngrams)==3:\n",
    "        clasifier=first_line.split(\"with\")[1].strip()\n",
    "        ngram.append(ngrams[0][-6:])\n",
    "        ngram.append(ngrams[1])\n",
    "        ngram.append(ngrams[2][:6])\n",
    "        first_line=first_line.split(\" \")\n",
    "    else:\n",
    "        first_line=first_line.split(\" \")\n",
    "        ngram.append(first_line[1])\n",
    "        clasifier=first_line[3]\n",
    "    \n",
    "    feature=first_line[0]\n",
    "    precision=one_line[6].split(\" \")[9]\n",
    "    recall=one_line[6].split(\" \")[15]\n",
    "    f1score=one_line[6].split(\" \")[21]\n",
    "    accuray=one_line[8].strip()\n",
    "    for i,k in enumerate(ngram):\n",
    "        ngram[i]=k.split(\"-\")[0]\n",
    "#     +(\"%.2f\" % round(float(accuray),2))\n",
    "    if feature==\"characters\" and (ngram[0]==\"2\" ):\n",
    "        print(\"\\hline\")\n",
    "        print(classifiers_names[clasifier]+\" & \"+\",\".join(ngram)+\"-gram\"+\" & \"+precision+\" & \"+recall+\" & \"+f1score+\" \\\\\\\\\")\n",
    "#         print(feature,ngram,clasifier,accuray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\\hline\n",
    "Linear SVM (SVC) & 1-gram & 0.95 & 0.95 & 0.95 & 0.97 & 0.96 & 0.96 &  \\\\\n",
    "\\hline\n",
    "Linear SVM (SVC) & 1,2-gram & 0.96 & 0.96 & 0.96 & 0.98 & 0.98 & 0.98 &  \\\\\n",
    "\\hline\n",
    "Linear SVM (SVC) & 1,2,3-gram & 0.97 & 0.97 & 0.97 & 0.98 & 0.98 & 0.98 &  \\\\\n",
    "\\hline\n",
    "Multinomial NB & 1-gram & 0.95 & 0.94 & 0.94 & 0.97 & 0.96 & 0.96 &  \\\\\n",
    "\\hline\n",
    "Multinomial NB & 1,2-gram & 0.96 & 0.96 & 0.96 & 0.98 & 0.97 & 0.97 &  \\\\\n",
    "\\hline\n",
    "Multinomial NB & 1,2,3-gram & 0.96 & 0.96 & 0.96 & 0.98 & 0.97 & 0.97 &  \\\\\n",
    "\\hline\n",
    "Random Forest & 1-gram & 0.90 & 0.89 & 0.89 & 0.93 & 0.92 & 0.92 &  \\\\\n",
    "\\hline\n",
    "Random Forest & 1,2-gram & 0.91 & 0.90 & 0.90 & 0.94 & 0.93 & 0.93 &  \\\\\n",
    "\\hline\n",
    "Random Forest & 1,2,3-gram & 0.90 & 0.89 & 0.89 & 0.92 & 0.90 & 0.90 &  \\\\\n",
    "\\hline\n",
    "AdaBoost & 1-gram &0.88 & 0.88 & 0.88 & 0.26 & 0.46 & 0.33 &  \\\\\n",
    "\\hline\n",
    "AdaBoost & 1,2-gram &0.88 & 0.88 & 0.88 & 0.26 & 0.46 & 0.33 &  \\\\\n",
    "\\hline\n",
    "AdaBoost & 1,2,3-gram &0.88 & 0.88 & 0.88 & 0.26 & 0.46 & 0.33 &  \\\\\n",
    "\\hline\n",
    "Decision Tree & 1-gram &0.80 & 0.79 & 0.79 & 0.92 & 0.91 & 0.91 &  \\\\\n",
    "\\hline\n",
    "Gaussian NB & 1-gram &0.89 & 0.88 & 0.88 &  0.98 & 0.97 & 0.97 &  \\\\\n",
    "\\hline\n",
    "Multi-layer Perc. (MLP) & 1-gram &0.97 & 0.97 & 0.97 & 0.27 & 0.51 & 0.35 &  \\\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(range(0,10,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_line=BBC_BBC_lines[9:9+9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_line[0].strip()[23:].split(\"and\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for line in lines:\n",
    "    if \"Forest\" in line:\n",
    "        print (line.split(\"and\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers_names={\"LinearSVC\":\"Linear SVM (SVC)\",\"MultinomialNB\":\"Multinomial NB\",\"RandomForestClassifier\":\"Random Forest\",\n",
    "                  \"AdaBoostClassifier\":\"AdaBoost\",\"DecisionTreeClassifier\":\"Decision Tree\",\"GaussianNB\":\"Gaussian NB\",\"MLPClassifier\":\"Multi-layer Perc. (MLP)\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HPCenv_conda",
   "language": "python",
   "name": "hpcenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
